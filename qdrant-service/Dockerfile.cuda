#
# CUDA runtime image for synaplan-qdrant-service with native ONNXRuntime embeddings.
#
# Notes:
# - This image is intentionally "fat": it targets max embedding performance on GPU.
# - Provide libonnxruntime.so (CUDA-enabled build) via the base image OR by copying it in.
# - Mount model/tokenizer into /models/bge-m3 (or configure env paths).
#

FROM rustlang/rust:nightly-slim AS builder
WORKDIR /app

RUN apt-get update && apt-get install -y \
    pkg-config \
    libssl-dev \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

COPY . .

# Build with native ONNX feature
RUN cargo build --release --features native_onnx

# Runtime: CUDA base image (choose CUDA version that matches your host driver)
FROM nvidia/cuda:12.4.1-cudnn-runtime-ubuntu22.04 AS runtime
WORKDIR /app

RUN apt-get update && apt-get install -y \
    ca-certificates \
    libssl3 \
    && rm -rf /var/lib/apt/lists/*

COPY --from=builder /app/target/release/synaplan-qdrant-service /app/synaplan-qdrant-service

# Expect libonnxruntime.so to be available in the image via:
# - apt-installed ORT (if you add it), or
# - copied into /usr/lib, or
# - mounted at runtime and pointed to with ORT_DYLIB_PATH.
#
# ENV ORT_DYLIB_PATH=/usr/lib/libonnxruntime.so

RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

EXPOSE 8090
ENTRYPOINT ["/app/synaplan-qdrant-service"]


